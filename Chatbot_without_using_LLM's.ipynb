{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNUuFy9F/xR0H0XuNS8g6ql",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravinnd3/Chatbot-without-LLM-s/blob/main/Chatbot_without_using_LLM's.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk\n"
      ],
      "metadata": {
        "id": "0m4J-b8reI47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "Q-s2jVYwx0G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33VUILUqwhT6"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import re\n",
        "import contractions\n",
        "from collections import Counter\n",
        "\n",
        "import pickle\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')                    # Tokenizer\n",
        "nltk.download('tagsets')                    #tag info\n",
        "nltk.download('punkt_tab') # Download punkt_tab for tokenization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # POS tagger\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W9-W1wtRwzTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download latest version (will be downloaded to /kaggle/input/...)\n",
        "path = kagglehub.dataset_download(\"grafstor/simple-dialogs-for-chatbot\")\n",
        "\n",
        "# Define the desired destination path\n",
        "destination_path = \"/content/chatbot\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Copy the contents of the downloaded dataset to the destination path\n",
        "# We use shell command for simplicity, you could also use shutil.copytree\n",
        "!cp -r \"{path}/.\" \"{destination_path}/\"\n",
        "\n",
        "print(f\"Dataset copied to: {destination_path}\")\n",
        "\n",
        "# List files in the destination directory to confirm\n",
        "!ls \"{destination_path}\""
      ],
      "metadata": {
        "id": "KdHGL4fIxFp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/chatbot/dialogs.txt\",sep='\\t',header=None,names=['question','answer'])\n"
      ],
      "metadata": {
        "id": "rrCEYe4Dxd9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "S1PrQJi9xqOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ZXgZURP2yDIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "5R76YcbbyMoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "erDx7oYyyRSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking Sentence length**"
      ],
      "metadata": {
        "id": "LWxuZAsM2lki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['q_len'] = data['question'].apply(lambda x: len(x.split()))\n",
        "data['a_len'] = data['answer'].apply(lambda x: len(x.split()))\n"
      ],
      "metadata": {
        "id": "PJdEeP2k2eM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[['q_len','a_len']].hist(bins=20)\n"
      ],
      "metadata": {
        "id": "ayD0thWh2eKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vocabulary analysis**"
      ],
      "metadata": {
        "id": "DwvhcK0jVHiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "all_text = ' '.join(data['question']) + ' ' + ' '.join(data['answer'])\n",
        "word_counts = Counter(all_text.split())\n",
        "\n",
        "print(\"Vocabulary size:\", len(word_counts))\n",
        "print(\"Most common words:\", word_counts.most_common(20))\n"
      ],
      "metadata": {
        "id": "DZ4w_pJA2eG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['q_len','a_len'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "nTNc3X-3V7Sa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cleaning Text**"
      ],
      "metadata": {
        "id": "qjt6FwFBVzUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r\"[^a-zA-Z0-9?!.']\", \" \", text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "data['question'] = data['question'].apply(clean_text)\n",
        "data['answer'] = data['answer'].apply(clean_text)"
      ],
      "metadata": {
        "id": "SmTK4-vb06MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Dg8c4puoaNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "GD4QI3-Uo65D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8sD8oeAyya7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "METHOD 1: Retrieval-Based Chatbot (No Deep Learning Needed)\n",
        "\n",
        "When a user asks something, the chatbot:\n",
        "\n",
        "Finds the most similar question in your dataset.\n",
        "\n",
        "Returns the paired answer."
      ],
      "metadata": {
        "id": "if1fPe6cIabG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF model\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(data['question'])\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    user_tfidf = vectorizer.transform([user_input])\n",
        "    similarities = cosine_similarity(user_tfidf, tfidf)\n",
        "    idx = similarities.argmax()\n",
        "    return data.iloc[idx]['answer']\n",
        "\n",
        "print(\"Chatbot: Hi! Type 'bye' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \").lower()\n",
        "    if user_input == 'bye':\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = chatbot_response(user_input)\n",
        "    print(\"Chatbot:\", response)\n"
      ],
      "metadata": {
        "id": "PLlPoWjwIWum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M1rARZ34IWsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "METHOD 2: Generative Chatbot (Seq2Seq using LSTM)\n",
        "If you want the chatbot to generate new sentences (not just pick from known ones), use a Seq2Seq neural network.\n",
        "🧮 Steps\n",
        "Step 1: Preprocess\n",
        "Tokenize questions and answers\n",
        "Add <start> and <end> tokens to answers\n",
        "Pad sequences to same length\n",
        "Step 2: Train Encoder–Decoder LSTM\n",
        "Encoder: reads the question\n",
        "Decoder: generates the answer, word by word\n",
        "Step 3: Predict response\n",
        "Given a user query → encode it → decode word by word until <end>\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TQ5gXvVJJWx4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tIAQ852snZnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add start and end tokens\n",
        "data['answer'] = data['answer'].apply(lambda x: '<start> ' + x + ' <end>')\n",
        "\n"
      ],
      "metadata": {
        "id": "QXkEdpt5IWqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "dfgVPY92J4dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tokenizer = Tokenizer(filters='', oov_token=None)\n",
        "tokenizer.fit_on_texts(list(data['question']) + list(data['answer']))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "mNanEEDcJz4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "id": "4jLUYiZRJ-uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "🚀 Which Should You Choose?\n",
        "Approach\tGood For\tNeeds GPU?\tLearns new replies?\n",
        "Retrieval (TF-IDF)\tSmall data, FAQs\t❌ No\t❌ No\n",
        "Seq2Seq (LSTM)\tConversational data\t⚙️ Optional (faster with GPU)\t✅ Yes\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "PPkkCI6QLnSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to sequences\n",
        "X = tokenizer.texts_to_sequences(data['question'])\n",
        "y = tokenizer.texts_to_sequences(data['answer'])\n",
        "\n"
      ],
      "metadata": {
        "id": "PtfOlZLJULxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding\n",
        "max_len = max(max(len(seq) for seq in X), max(len(seq) for seq in y))\n",
        "X = pad_sequences(X, maxlen=max_len, padding='post')\n",
        "y = pad_sequences(y, maxlen=max_len, padding='post')\n",
        "\n"
      ],
      "metadata": {
        "id": "l0k-tacXY1m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder inputs/outputs\n",
        "y_input = y[:, :-1]\n",
        "y_output = y[:, 1:]\n"
      ],
      "metadata": {
        "id": "AYT35ZC1ZAUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build seq2seq model**"
      ],
      "metadata": {
        "id": "CqwkGDdKZMNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "lstm_units = 256\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_len,), name=\"encoder_input\")\n",
        "enc_emb = Embedding(vocab_size, embed_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True, name=\"encoder_lstm\")\n",
        "_, state_h, state_c = encoder_lstm(enc_emb)\n",
        "encoder_states = [state_h, state_c]\n"
      ],
      "metadata": {
        "id": "nXBzz_-SZQb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len-1,), name=\"decoder_input\")\n",
        "dec_emb = Embedding(vocab_size, embed_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, name=\"decoder_lstm\")\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax', name=\"decoder_dense\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "Q9KRmLDGZTUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "LrBvFNMEZZAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model training\n",
        "model.fit([X, y_input], np.expand_dims(y_output, -1), batch_size=64, epochs=200,verbose=0)\n"
      ],
      "metadata": {
        "id": "m64A6OBvZoKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer and model\n",
        "pickle.dump(tokenizer, open(\"tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "zVyN5_AEaApw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"seq2seq_chatbot.h5\")"
      ],
      "metadata": {
        "id": "k6YCdXabaDj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Inference Models (seq2seq)**\n"
      ],
      "metadata": {
        "id": "DYzdSYTXaJbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "tokenizer = pickle.load(open(\"tokenizer.pkl\", \"rb\"))\n",
        "model = load_model(\"seq2seq_chatbot.h5\")"
      ],
      "metadata": {
        "id": "BaF29wASaRIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder inference\n",
        "encoder_inputs_inf = model.input[0]\n",
        "encoder_emb_layer = model.get_layer(\"encoder_embedding\")\n",
        "encoder_lstm_layer = model.get_layer(\"encoder_lstm\")\n",
        "enc_emb_inf = encoder_emb_layer(encoder_inputs_inf)\n",
        "_, state_h_enc, state_c_enc = encoder_lstm_layer(enc_emb_inf)\n",
        "encoder_model = Model(encoder_inputs_inf, [state_h_enc, state_c_enc])"
      ],
      "metadata": {
        "id": "Pka0pKcOaT-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder inference\n",
        "decoder_inputs_inf = Input(shape=(1,), name=\"decoder_input_infer\")\n",
        "decoder_emb_layer = model.get_layer(\"decoder_embedding\")\n",
        "decoder_lstm_layer = model.get_layer(\"decoder_lstm\")\n",
        "decoder_dense_layer = model.get_layer(\"decoder_dense\")\n",
        "\n",
        "decoder_state_input_h = Input(shape=(lstm_units,), name=\"decoder_state_input_h\")\n",
        "decoder_state_input_c = Input(shape=(lstm_units,), name=\"decoder_state_input_c\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "dec_emb2 = decoder_emb_layer(decoder_inputs_inf)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm_layer(\n",
        "    dec_emb2, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_outputs2 = decoder_dense_layer(decoder_outputs2)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs_inf] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + [state_h2, state_c2]\n",
        ")"
      ],
      "metadata": {
        "id": "z43CS0akaX3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse tokenizer\n",
        "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "Ct8Gg8Rwace_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sentence_to_seq(sentence, max_len=max_len):\n",
        "    seq = tokenizer.texts_to_sequences([sentence.lower()])\n",
        "    return pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "\n",
        "def decode_sequence(input_text, max_output_len=15):\n",
        "    states_value = encoder_model.predict(sentence_to_seq(input_text))\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(max_output_len):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
        "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        if sampled_word in ('<end>', ''):\n",
        "            break\n",
        "\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip().capitalize()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "214_DQRVSRag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat\n",
        "print(\"Chatbot is ready! Type 'bye' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'bye':\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "    reply = decode_sequence(user_input)\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "HaKX0xinIWd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\n",
        "    \"how are you?\",\n",
        "    \"are you right handed?\",\n",
        "    \"i am pretty good. thanks for asking.\",\n",
        "    \"no problem. so how have you been?\",\n",
        "    \"but i do all my writing with my right hand.\"\n",
        "]\n",
        "\n",
        "for s in samples:\n",
        "    print(f\"You: {s}\")\n",
        "    print(f\"Bot: {decode_sequence(s)}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "id": "or_MkXghzrO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EfhBps2EzrL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E1NVThNMzrGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2NkOxV3gzrDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "13krVCePzqvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Context Window - to remmember last 3 conversation"
      ],
      "metadata": {
        "id": "rvGgz01QZKNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
        "encoder_input_len = 21\n",
        "context_window = 3  # remember last 3 messages (user+bot)\n",
        "\n",
        "def sentence_to_seq(sentence):\n",
        "    seq = tokenizer.texts_to_sequences([sentence.lower()])\n",
        "    return pad_sequences(seq, maxlen=encoder_input_len, padding='post')\n",
        "\n",
        "def decode_sequence(input_text):\n",
        "    # Encode input text\n",
        "    states_value = encoder_model.predict(sentence_to_seq(input_text))\n",
        "\n",
        "    # Initialize target sequence with start token\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = tokenizer.word_index.get('<start>', 1)\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(encoder_input_len - 1):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
        "\n",
        "        # Break if <end> or nothing predicted\n",
        "        if sampled_word.lower() in ('<end>', 'end', ''):\n",
        "            break\n",
        "\n",
        "        decoded_sentence += ' ' + sampled_word\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    # Clean and return nicely formatted output\n",
        "    decoded_sentence = decoded_sentence.strip()\n",
        "    return decoded_sentence.capitalize()\n",
        "\n",
        "#Context Memory\n",
        "context_history = []  # stores conversation turns\n",
        "\n",
        "print(\"Context-Aware Chatbot is ready! Type 'quit' to exit.\\n\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'bye':\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Build contextual input using last few turns\n",
        "    context_str = \" \".join(context_history[-context_window:]) + \" \" + user_input\n",
        "    reply = decode_sequence(context_str)\n",
        "\n",
        "    print(\"Bot:\", reply)\n",
        "\n",
        "    # Save context (keep conversation memory)\n",
        "    context_history.append(f\"user: {user_input}\")\n",
        "    context_history.append(f\"bot: {reply}\")\n"
      ],
      "metadata": {
        "id": "U6Wkn19mIWbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xCiln8XIIWYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qRqhJDA3IWVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**With POS Tagging**"
      ],
      "metadata": {
        "id": "HltggrSOal6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade nltk"
      ],
      "metadata": {
        "id": "lh3oFyrLXpuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences = list(data['question']) + list(data['answer'])"
      ],
      "metadata": {
        "id": "lrTJTcnBZ9rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos_tags_set = set()\n",
        "# for sent in all_sentences:\n",
        "#     tags = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(sent))]\n",
        "#     pos_tags_set.update(tags)\n",
        "\n",
        "# pos2idx = {tag: i+1 for i, tag in enumerate(sorted(pos_tags_set))}\n",
        "# pos_vocab_size = len(pos2idx) + 1  # +1 for padding\n",
        "# pos_vocab_size"
      ],
      "metadata": {
        "id": "unWfotZ4ZMjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = list(data['question'])\n",
        "answers   = list(data['answer'])\n"
      ],
      "metadata": {
        "id": "UX1m8jgnZcfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add start and end tokens\n",
        "answers = [\"<start> \" + ans + \" <end>\" for ans in answers]\n"
      ],
      "metadata": {
        "id": "B2rjYJ1Ac4Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "mZQ6-0ARc5Sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Word tokenizer ---\n",
        "tokenizer = Tokenizer(filters='', oov_token=None)\n",
        "tokenizer.fit_on_texts([\"<start>\", \"<end>\"] + questions + answers)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "hyohpM7-c8yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# POS tokenizer\n",
        "all_sentences = questions + answers\n",
        "pos_tags_set = set()\n",
        "for sent in all_sentences:\n",
        "    tags = [tag for word, tag in nltk.pos_tag(nltk.word_tokenize(sent))]\n",
        "    pos_tags_set.update(tags)\n",
        "\n",
        "pos2idx = {tag: i+1 for i, tag in enumerate(sorted(pos_tags_set))}\n",
        "pos_vocab_size = len(pos2idx) + 1\n",
        "pos_embed_dim = 32"
      ],
      "metadata": {
        "id": "KMuvqBzDdB9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_len = 20\n",
        "embed_dim = 128\n",
        "lstm_units = 256"
      ],
      "metadata": {
        "id": "BQ2qmP0AgCfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_to_seq_and_pos(sentence):\n",
        "    seq = tokenizer.texts_to_sequences([sentence.lower()])\n",
        "    seq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
        "\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    pos_seq = [pos2idx.get(tag, 0) for word, tag in nltk.pos_tag(words)]\n",
        "    pos_seq = pad_sequences([pos_seq], maxlen=max_len, padding='post')\n",
        "\n",
        "    return seq, pos_seq"
      ],
      "metadata": {
        "id": "YXdlSNOlhyz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "encoder_input_words_array = []\n",
        "encoder_input_pos_array   = []\n",
        "decoder_input_array       = []\n",
        "decoder_target_array      = []\n",
        "\n",
        "for i in range(len(questions)):\n",
        "    w_seq, p_seq = sentence_to_seq_and_pos(questions[i])\n",
        "    encoder_input_words_array.append(w_seq[0])\n",
        "    encoder_input_pos_array.append(p_seq[0])\n",
        "\n",
        "    ans_seq = tokenizer.texts_to_sequences([answers[i]])[0]\n",
        "    decoder_input_array.append(ans_seq[:-1])   # input: <start> ... last word\n",
        "    decoder_target_array.append(ans_seq[1:])   # target: first word ... <end>\n",
        "\n",
        "encoder_input_words_array = pad_sequences(encoder_input_words_array, maxlen=max_len, padding='post')\n",
        "encoder_input_pos_array   = pad_sequences(encoder_input_pos_array, maxlen=max_len, padding='post')\n",
        "decoder_input_array       = pad_sequences(decoder_input_array, maxlen=max_len, padding='post')\n",
        "decoder_target_array      = pad_sequences(decoder_target_array, maxlen=max_len, padding='post')\n",
        "decoder_target_array      = np.expand_dims(decoder_target_array, -1)\n",
        "\n",
        "#Encoder\n",
        "encoder_input_words = Input(shape=(max_len,), name=\"encoder_words_input\")\n",
        "encoder_input_pos   = Input(shape=(max_len,), name=\"encoder_pos_input\")\n",
        "\n",
        "word_emb = Embedding(vocab_size, embed_dim, mask_zero=False)(encoder_input_words)\n",
        "pos_emb  = Embedding(pos_vocab_size, pos_embed_dim, mask_zero=False)(encoder_input_pos)\n",
        "\n",
        "encoder_emb = Concatenate()([word_emb, pos_emb])\n",
        "encoder_outputs, state_h, state_c = LSTM(lstm_units, return_state=True)(encoder_emb)\n",
        "encoder_states = [state_h, state_c]"
      ],
      "metadata": {
        "id": "518uEcH5h9W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "decoder_inputs = Input(shape=(max_len,), name=\"decoder_input\")\n",
        "decoder_emb = Embedding(vocab_size, embed_dim, mask_zero=False, name=\"decoder_embedding\")(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_emb, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "EZziialQiCXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_input_words, encoder_input_pos, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "l5cNj-zJiErW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "model.fit([encoder_input_words_array, encoder_input_pos_array, decoder_input_array],\n",
        "          decoder_target_array,\n",
        "          batch_size=32,\n",
        "          epochs=100,\n",
        "          validation_split=0.1)"
      ],
      "metadata": {
        "id": "D48me-ryiIuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Models\n",
        "encoder_model = Model([encoder_input_words, encoder_input_pos], encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(lstm_units,), name=\"decoder_state_input_h\")\n",
        "decoder_state_input_c = Input(shape=(lstm_units,), name=\"decoder_state_input_c\")\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_inputs_infer = Input(shape=(1,), name=\"decoder_input_infer\")\n",
        "decoder_emb_infer = model.get_layer(\"decoder_embedding\")(decoder_inputs_infer)\n",
        "decoder_lstm_infer = LSTM(lstm_units, return_sequences=False, return_state=True, name=\"decoder_lstm_infer\")\n",
        "dec_outputs, state_h2, state_c2 = decoder_lstm_infer(decoder_emb_infer, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h2, state_c2]\n",
        "decoder_outputs = decoder_dense(dec_outputs)\n",
        "decoder_model = Model([decoder_inputs_infer] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "#Reverse word index\n",
        "reverse_word_index = {v:k for k,v in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "AVBz5nWdkdjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decode sequence\n",
        "def decode_sequence(input_text):\n",
        "    w_seq, p_seq = sentence_to_seq_and_pos(input_text)\n",
        "    states_value = encoder_model.predict([w_seq, p_seq])\n",
        "\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = tokenizer.word_index['<start>']\n",
        "\n",
        "    decoded_sentence = ''\n",
        "    for _ in range(max_len):\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0])\n",
        "        sampled_word = reverse_word_index.get(sampled_token_index, '')\n",
        "        if sampled_word in (\"<end>\", \"\"):\n",
        "            break\n",
        "        if sampled_word != \"<start>\":\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "        target_seq[0,0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip().capitalize()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BS35bxcBZcdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat\n",
        "context_history = []\n",
        "context_window = 3\n",
        "\n",
        "print(\"Chatbot is ready! Type 'quit' to exit.\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'bye':\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "    context_str = \" \".join(context_history[-context_window:]) + \" \" + user_input\n",
        "    reply = decode_sequence(context_str)\n",
        "    print(\"Bot:\", reply)\n",
        "    context_history.append(user_input)\n",
        "    context_history.append(reply)"
      ],
      "metadata": {
        "id": "0Ea0j9upZcat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "Hj6iqLfgZcYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNWEz--AZcVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vr2qPMHh49c0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}